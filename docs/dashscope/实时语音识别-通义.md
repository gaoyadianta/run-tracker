在直播、在线会议、语音聊天或智能助手等场景中，需要将连续的音频流实时转化为文字，以提供即时字幕、生成会议记录或响应语音指令。通义千问实时语音识别服务能够接收音频流并实时转写。

核心功能
多语种高精度识别：支持多语言高精度语音识别（涵盖普通话及多种方言，如粤语、四川话等，详情请参见模型功能特性）

复杂环境适应：具备应对复杂声学环境的能力，支持自动语种检测与智能非人声过滤

上下文增强：通过配置上下文提高识别准确率（比传统的热词方案更灵活、强大）

情感识别：支持多种情绪状态的识别（涵盖惊讶、平静、愉快、悲伤、厌恶、愤怒和恐惧）

适用范围
支持的模型：

中国内地国际
在中国内地部署模式下，接入点与数据存储均位于北京地域，模型推理计算资源仅限于中国内地。

调用以下模型时，请选择北京地域的API Key：

通义千问3-ASR-Flash-Realtime：qwen3-asr-flash-realtime（稳定版，当前等同qwen3-asr-flash-realtime-2025-10-27）、qwen3-asr-flash-realtime-2025-10-27（快照版）

更多信息请参见模型列表

模型选型



场景

推荐模型

理由

智能客服质检

qwen3-asr-flash-realtime、qwen3-asr-flash-realtime-2025-10-27

实时分析通话内容与客户情绪，辅助坐席并进行服务质量监控

直播/短视频

为直播内容生成实时字幕，覆盖多语种观众

在线会议/访谈

实时记录会议发言，快速生成文字纪要，提高信息整理效率

专业领域转写（医疗/法律等）

支持上下文增强，动态注入领域热词提升命中率

更多说明请参见模型功能特性。

快速开始
使用DashScope SDK使用WebSocket API
以下示例演示如何通过 WebSocket 连接发送本地音频文件并获取识别结果。

获取API Key：获取API Key，安全起见，推荐将API Key配置到环境变量。

编写并运行代码：通过代码实现认证、连接、发送音频和接收结果的完整流程（详情请参见交互流程）。

PythonJavaNode.js
在运行示例前，请确保已使用以下命令安装依赖：

 
pip uninstall websocket-client
pip uninstall websocket
pip install websocket-client
请不要将示例代码文件命名为 websocket.py，否则可能触发如下错误：AttributeError: module 'websocket' has no attribute 'WebSocketApp'. Did you mean: 'WebSocket'?

 
# pip install websocket-client
import os
import time
import json
import threading
import base64
import websocket
import logging
import logging.handlers
from datetime import datetime

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

# 新加坡和北京地域的API Key不同。获取API Key：https://help.aliyun.com/zh/model-studio/get-api-key
# 若没有配置环境变量，请用百炼API Key将下行替换为：API_KEY="sk-xxx"
API_KEY = os.environ.get("DASHSCOPE_API_KEY", "sk-xxx")
QWEN_MODEL = "qwen3-asr-flash-realtime"
# 以下是北京地域baseUrl，如果使用新加坡地域的模型，需要将baseUrl替换为：wss://dashscope-intl.aliyuncs.com/api-ws/v1/realtime
baseUrl = "wss://dashscope.aliyuncs.com/api-ws/v1/realtime"
url = f"{baseUrl}?model={QWEN_MODEL}"
print(f"Connecting to server: {url}")

# 注意： 如果是非vad模式，建议持续发送的音频时长累加不超过60s
enableServerVad = True
is_running = True  # 增加运行标志位

headers = [
    "Authorization: Bearer " + API_KEY,
    "OpenAI-Beta: realtime=v1"
]

def init_logger():
    formatter = logging.Formatter('%(asctime)s|%(levelname)s|%(message)s')
    f_handler = logging.handlers.RotatingFileHandler(
        "omni_tester.log", maxBytes=100 * 1024 * 1024, backupCount=3
    )
    f_handler.setLevel(logging.DEBUG)
    f_handler.setFormatter(formatter)

    console = logging.StreamHandler()
    console.setLevel(logging.DEBUG)
    console.setFormatter(formatter)

    logger.addHandler(f_handler)
    logger.addHandler(console)

def on_open(ws):
    logger.info("Connected to server.")

    # 会话更新事件
    event_manual = {
        "event_id": "event_123",
        "type": "session.update",
        "session": {
            "modalities": ["text"],
            "input_audio_format": "pcm",
            "sample_rate": 16000,
            "input_audio_transcription": {
                # 语种标识，可选，如果有明确的语种信息，建议设置
                "language": "zh"
                # 语料，可选，如果有语料，建议设置以增强识别效果
                # "corpus": {
                #     "text": ""
                # }
            },
            "turn_detection": None
        }
    }
    event_vad = {
        "event_id": "event_123",
        "type": "session.update",
        "session": {
            "modalities": ["text"],
            "input_audio_format": "pcm",
            "sample_rate": 16000,
            "input_audio_transcription": {
                "language": "zh"
            },
            "turn_detection": {
                "type": "server_vad",
                "threshold": 0.0,
                "silence_duration_ms": 400
            }
        }
    }
    if enableServerVad:
        logger.info(f"Sending event: {json.dumps(event_vad, indent=2)}")
        ws.send(json.dumps(event_vad))
    else:
        logger.info(f"Sending event: {json.dumps(event_manual, indent=2)}")
        ws.send(json.dumps(event_manual))

def on_message(ws, message):
    global is_running
    try:
        data = json.loads(message)
        logger.info(f"Received event: {json.dumps(data, ensure_ascii=False, indent=2)}")
        if data.get("type") == "session.finished":
            logger.info(f"Final transcript: {data.get('transcript')}")
            logger.info("Closing WebSocket connection after session finished...")
            is_running = False  # 停止音频发送线程
            ws.close()
    except json.JSONDecodeError:
        logger.error(f"Failed to parse message: {message}")

def on_error(ws, error):
    logger.error(f"Error: {error}")

def on_close(ws, close_status_code, close_msg):
    logger.info(f"Connection closed: {close_status_code} - {close_msg}")

def send_audio(ws, local_audio_path):
    time.sleep(3)  # 等待会话更新完成
    global is_running

    with open(local_audio_path, 'rb') as audio_file:
        logger.info(f"文件读取开始: {datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
        while is_running:
            audio_data = audio_file.read(3200)  # ~0.1s PCM16/16kHz
            if not audio_data:
                logger.info(f"文件读取完毕: {datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
                if ws.sock and ws.sock.connected:
                    if not enableServerVad:
                        commit_event = {
                            "event_id": "event_789",
                            "type": "input_audio_buffer.commit"
                        }
                        ws.send(json.dumps(commit_event))
                    finish_event = {
                        "event_id": "event_987",
                        "type": "session.finish"
                    }
                    ws.send(json.dumps(finish_event))
                break

            if not ws.sock or not ws.sock.connected:
                logger.info("WebSocket已关闭，停止发送音频。")
                break

            encoded_data = base64.b64encode(audio_data).decode('utf-8')
            eventd = {
                "event_id": f"event_{int(time.time() * 1000)}",
                "type": "input_audio_buffer.append",
                "audio": encoded_data
            }
            ws.send(json.dumps(eventd))
            logger.info(f"Sending audio event: {eventd['event_id']}")
            time.sleep(0.1)  # 模拟实时采集

# 初始化日志
init_logger()
logger.info(f"Connecting to WebSocket server at {url}...")

local_audio_path = "your_audio_file.pcm"
ws = websocket.WebSocketApp(
    url,
    header=headers,
    on_open=on_open,
    on_message=on_message,
    on_error=on_error,
    on_close=on_close
)

thread = threading.Thread(target=send_audio, args=(ws, local_audio_path))
thread.start()
ws.run_forever()
核心用法：上下文增强
通过提供上下文（Context），可对特定领域的专有词汇（如人名、地名、产品术语）进行识别优化，显著提升转写准确率。此功能远比传统的热词方案更灵活、强大。

长度限制：Context内容不超过 10000 Token。

用法：

WebSocket API：通过session.update事件的session.input_audio_transcription.corpus.text参数设置。

Python SDK：通过corpus_text参数设置。

Java SDK：通过corpusText参数设置。

支持的文本类型：包括（但不限于）

热词列表（多种分隔符格式，如热词 1、热词 2、热词 3、热词 4）

任意格式与长度的文本段落或篇章

混合内容：词表与段落的任意组合

无关或无意义文本（包括乱码，对无关文本的容错性极高，几乎不会受到负面影响）

示例：

某段音频正确识别结果应该为“投行圈内部的那些黑话，你了解哪些？首先，外资九大投行，Bulge Bracket，BB ...”。

不使用上下文增强

未使用上下文增强时，部分投行公司名称识别有误，例如 “Bird Rock” 正确应为 “Bulge Bracket”。

识别结果：“投行圈内部的那些黑话，你了解哪些？首先，外资九大投行，Bird Rock，BB ...”

使用上下文增强

使用上下文增强，对投行公司名称识别正确。

识别结果：“投行圈内部的那些黑话，你了解哪些？首先，外资九大投行，Bulge Bracket，BB ...”

实现上述效果，可在上下文中加入以下任一内容：

词表：

词表1：

 
Bulge Bracket、Boutique、Middle Market、国内券商
词表2：

 
Bulge Bracket Boutique Middle Market 国内券商
词表3：

 
['Bulge Bracket', 'Boutique', 'Middle Market', '国内券商']
自然语言：

 
投行分类大揭秘！
最近有不少澳洲的小伙伴问我，到底什么是投行？今天就来给大家科普一下，对于留学生来说，投行主要可以分为四大类：Bulge Bracket、Boutique、Middle Market和国内券商。
Bulge Bracket投行：这就是我们常说的九大投行，包括高盛、摩根士丹利等。这些大行在业务范围和规模上都相当庞大。
Boutique投行：这些投行规模相对较小，但业务领域非常专注。比如Lazard、Evercore等，它们在特定领域有着深厚的专业知识和经验。
Middle Market投行：这类投行主要服务于中型公司，提供并购、IPO等业务。虽然规模不如大行，但在特定市场上有很高的影响力。
国内券商：随着中国市场的崛起，国内券商在国际市场上也扮演着越来越重要的角色。
此外，还有一些Position和business的划分，大家可以参考相关的图表。希望这些信息能帮助大家更好地了解投行，为未来的职业生涯做好准备！
有干扰的自然语言：有些文本和识别内容无关，例如下面这个示例里的人名

 
投行分类大揭秘！
最近有不少澳洲的小伙伴问我，到底什么是投行？今天就来给大家科普一下，对于留学生来说，投行主要可以分为四大类：Bulge Bracket、Boutique、Middle Market和国内券商。
Bulge Bracket投行：这就是我们常说的九大投行，包括高盛、摩根士丹利等。这些大行在业务范围和规模上都相当庞大。
Boutique投行：这些投行规模相对较小，但业务领域非常专注。比如Lazard、Evercore等，它们在特定领域有着深厚的专业知识和经验。
Middle Market投行：这类投行主要服务于中型公司，提供并购、IPO等业务。虽然规模不如大行，但在特定市场上有很高的影响力。
国内券商：随着中国市场的崛起，国内券商在国际市场上也扮演着越来越重要的角色。
此外，还有一些Position和business的划分，大家可以参考相关的图表。希望这些信息能帮助大家更好地了解投行，为未来的职业生涯做好准备！
王皓轩 李梓涵 张景行 刘欣怡 陈俊杰 杨思远 赵雨桐 黄志强 周子墨 吴雅静 徐若曦 孙浩然 胡瑾瑜 朱晨曦 郭文博 何静姝 高宇航 林逸飞 
郑晓燕 梁博文 罗佳琪 宋明哲 谢婉婷 唐子骞 韩梦瑶 冯毅然 曹沁雪 邓子睿 萧望舒 许嘉树 
程一诺 袁芷若 彭浩宇 董思淼 范景玉 苏子衿 吕文轩 蒋诗涵 丁沐宸 
魏书瑶 任天佑 姜亦辰 华清羽 沈星河 傅瑾瑜 姚星辰 钟灵毓 阎立诚 金若水 陶然亭 戚少商 薛芷兰 邹云帆 熊子昂 柏文峰 易千帆
API参考
实时语音识别-通义千问API参考

模型功能特性


功能/特性

qwen3-asr-flash-realtime、qwen3-asr-flash-realtime-2025-10-27

支持语言

中文（普通话、四川话、闽南语、吴语、粤语）、英语、日语、德语、韩语、俄语、法语、葡萄牙语、阿拉伯语、意大利语、西班牙语、印地语、印尼语、泰语、土耳其语、乌克兰语、越南语、捷克语、丹麦语、菲律宾语、芬兰语、冰岛语、马来语、挪威语、波兰语、瑞典语

支持的音频格式

pcm、opus

采样率

8kHz、16kHz

声道

单声道

输入形式

二进制音频流

音频大小/时长

不限

情感识别

支持 固定开启

敏感词过滤

不支持

说话人分离

不支持

语气词过滤

不支持

时间戳

不支持

标点符号预测

支持 固定开启

上下文增强（比热词更强大）

支持 可配置

ITN（Inverse Text Normalization，逆文本正则化）

不支持

VAD（Voice Activity Detection，语音活动检测）

支持 固定开启

限流（RPS）

20

接入方式

Java/Python SDK、WebSocket API

价格

中国内地：0.00033元/秒

国际：0.00066元/秒

模型应用上架及备案
参见应用合规备案。